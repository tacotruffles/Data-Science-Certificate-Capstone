---
title: "mtv-eda"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library('tidyverse')
library('dplyr')
library('caret')
```

Now we've run the scraper on a random sample of ~700 albums
```{r, message = FALSE}
albums <- read_csv('data/album-cover-data.csv')

colSums(is.na(albums))
```

Only missing varibles in this data set are year. I've elected to not use year as a predicter so let's remove those, but before we do let's take a look at how much our albums are spread across the years. My thought is that since MTV started in the 80's that we won't see many albums before the early 80's and we should also get a nice distribution from there onward.
```{r}
dat <- albums %>%
  drop_na()

dat %>%
  ggplot() +
  geom_bar(aes(x=year))
```

That's what we expected....let's cut off albums before 1981, as that's the year MTV started and when serious marketing dollars when the age huge budgets on "artist development" and branding started. We'll also clean out non-studio releases (live, soundtracks, tours, etc.)
```{r}
dat_albums <- dat %>%
  filter(
    year >= 1980,
    genre != 'Non-Music' & genre != 'Classical' & genre !='Stage & Screen',
    !str_detect(title, 'Tour|Live'),
    !str_detect(title, 'New York')
    ) %>%
  mutate( decade = (year %/% 10) * 10) # group by decade

#dat_clean <- filter(dat_clean, !str_detect(title, 'Tour|Live')) # Remove more non-studio releases
# dat_clean <- filter(dat_clean, !str_detect(title, 'New York')) # Remove other live albums
# dat_clean <- filter(dat_clean, genre != 'Non-Music') # Remove non-studio release genres
# dat_clean <- filter(dat_clean, genre != 'Stage & Screen') # Remove non-studio release genres
# dat_clean <- filter(dat_clean, genre != 'Classical') # Remove non-studio release genres
# dat_clean <- filter(dat_clean, genre != 'Children\'s') # Remove non-studio release genres
#dat_clean <- mutate(dat_clean, genre = as.factor(genre))

dat_albums %>%
  ggplot() +
  geom_bar(aes(x=year))
```

```{r}
dat_albums %>% nrow()

dat_albums %>%
  group_by(genre) %>%
  count() %>%
  arrange(-n)
```

Now we're down to 556 albums
```{r}
dat_albums %>%
  mutate(genre = genre %>% fct_infreq() %>% fct_rev()) %>%
  ggplot() +
  geom_bar(aes(x=genre)) +
  coord_flip()
```

Not enough data to train onh Blues, Latin, Reggae
```{r}
dat_albums <- dat_albums %>%
  filter( genre != 'Blues' & genre != 'Reggae' & genre != 'Latin')
```

It's not suprising that there are a lot of Rock albums as the vast majority of Discogs albums are in the Rock genre. After cleaning our initial data to be just US studio releases and the 41 rows that were missing years, we're now left with **`r dat_clean %>% nrow()`** albums to peform our train/test split. This is a 2.6% random sample of **20861** albums.

```{r}
dat_albums %>% nrow()
```


```{r}
dat_albums %>%
  group_by(genre, year) %>%
  count() %>%
  ggplot(aes(x = year, y = n)) +
  geom_line(aes(col = genre))
```

Group genres by decade to teas out any variance in colors and captions.
```{r}

dat_albums %>%
  group_by(decade, genre) %>%
  count() %>%
  ggplot(aes(x = decade, y = n)) +
  geom_line(aes(col = genre))
```

Nicer distribution by decade eventhough we still get the massive rise of rock in the 2000's onward. Let's set up our Clean Data to train our model by removing the corellated `year` parameter, along with `artist` and `title`. We also need to spread our 1st caption column and drop the others to avoid collisions.

```{r}
# Step 1: Configure Class variable 
set.seed(123)
dat_clean <- dat_albums %>% 
  mutate(
    genre = as.factor(genre),
    caption_1 = as.factor(caption_1),
    n = 1,
    ) %>% 
  select(-title, -artist, -year, -caption_2, -caption_3, -caption_4, -caption_5) %>% # Remove unimportant variables
  spread(key = caption_1, value = n, fill = 0) %>%
  rename(Class = genre)

dat_clean <- dat_clean %>% mutate(id = row_number()) 
```

Let's split our data into Train/Test segements.
```{r}
# Step 2: Train Split
dat_train <- dat_clean %>%
 sample_frac(0.80)

dat_test <- dat_clean %>%
 anti_join(dat_train, by = 'id') %>%
 select(-id)

dat_train <- dat_train %>%
 select(-id)
```

We'll try without preprocessing first.
```{r}
# Step 3: Pre Process - Standarize Training Data
# mod_pp <- preProcess(dat_train, method = c('scale','center'))
# dat_train_pp <- predict(mod_pp, dat_train) 
dat_train_pp <- dat_train
```

Since we have uneven distributionn on our Target....let's upsample
```{r}
# Step 4: Resample as needed
dat_train_up <- upSample(x = dat_train_pp %>% select(-Class), y = dat_train_pp$Class)

dat_train_up %>%
  group_by(Class) %>%
  count()
```

Time to train now, ask questions later!
```{r}
# Step 5: Train Model
train_control <- trainControl(
  method = 'cv',
  number = 5
)

model_ranger = train(
  dat_train_up %>% select(-Class),
  dat_train_up$Class,
  method = "ranger",
  importance = "impurity",
  trControl = train_control,
)

model_ranger
```


```{r}
#Step 6: Get predictions from centered test data
# dat_test_pp <- predict(mod_pp, dat_test)
```

```{r}
# Step 7: Predict from centered test data (as standardized - w/ 'scale' option gives 'zero variance error' )
# predictions <- predict(model_ranger, dat_test_pp)

# Step 7: Predict from non-preProcessed test data => still get the same .292 accuracy!
predictions <- predict(model_ranger, dat_test)
```

```{r}
# Step 8: Anaylize Confusion matrix (Actuals => $Class)
confusionMatrix(predictions, dat_test$Class)
```

```{r, fig.height=20}
# Step 9: Variable importance
plot(varImp(model_ranger))
```

Turns out that 3x16 color samples, and number of faces are not great predicters for album genre. It would appear that album cover colors are too similar to make much of a difference. 

Perhaps some other ways to improve the data set would be to be to increase the complexity of colors? Or to add more caption colums to capture more variance.



