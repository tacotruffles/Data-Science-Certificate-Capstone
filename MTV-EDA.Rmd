---
title: "mtv-eda"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library('tidyverse')
library('dplyr')
```

```{r, message = FALSE}
mtv1 <- read_csv('data/10000-MTV-Music-Artists-page-1.csv')
mtv2 <- read_csv('data/10000-MTV-Music-Artists-page-2.csv')
mtv3 <- read_csv('data/10000-MTV-Music-Artists-page-3.csv')
mtv4 <- read_csv('data/10000-MTV-Music-Artists-page-4.csv')

mtv_dat <- rbind(mtv1, mtv2, mtv3, mtv4)

mtv_dat %>% nrow()

glimpse(mtv1)
glimpse(mtv2)
glimpse(mtv2)

```

```{r, heig}
mtv1 <- mtv1 %>% 
  select(name, genre) %>%
  drop_na()

mtv1 %>%
  group_by(genre) %>%
  count()

mtv1 %>%
  ggplot() +
  geom_bar(aes(x=genre)) +
  coord_flip()
```

```{r}
mtv_dat <- mtv_dat %>% 
  select(name, genre) %>%
  drop_na()

mtv_dat %>%
  ggplot() +
  geom_bar(aes(x=genre)) +
  coord_flip()

# 5115 Artists
mtv_dat %>% nrow()

# Genre EDA
mtv_genre <- mtv_dat %>%
  group_by(genre) %>%
  count() %>%
  arrange(-n)


#171 genres
nrow(mtv_genre)

# Top 10 Genres
mtv_top_genres <- mtv_dat %>%
  group_by(genre) %>%
  count() %>%
  arrange(-n) %>%
  head(10) %>%
  select(genre) %>%
  as_vector()

mtv_top_genres

mtv_dat_clean <- mtv_dat[mtv_dat$genre %in% mtv_top_genres,]

# 3435 Artists
nrow(mtv_dat_clean)

mtv_dat_clean %>%
  group_by(genre) %>%
  count() %>%
  arrange(-n)

mtv_dat_clean %>%
  ggplot(aes(x=genre)) +
  geom_bar()

mtv_artist_names <- mtv_dat_clean %>% select(name) %>% as_vector()

# Output artist names
fileConn<-file("data/artist-names-mtv-top-10-genres.txt")
writeLines(mtv_artist_names, fileConn)
close(fileConn)

```

Now we've run the scraper on a random sample of ~700 albums
```{r, message = FALSE}
albums <- read_csv('data/album-cover-data-mtv.csv')

colSums(is.na(albums))
```

Only missing varibles in this data set are year. I've elected to not use year as a predicter so let's remove those, but before we do let's take a look at how much our albums are spread across the years. My thought is that since MTV started in the 80's that we won't see many albums before the early 80's and we should also get a nice distribution from there onward.
```{r}
dat <- albums %>%
  drop_na()

dat %>%
  ggplot() +
  geom_bar(aes(x=year))
```

That's what we expected....let's cut off albums before 1981, as that's the year MTV started and when serious marketing dollars when the age huge budgets on "artist development" and branding started. We'll also clean out non-studio releases (live, soundtracks, tours, etc.)
```{r}
dat_clean <- dat %>%
  filter(
    year >= 1980,
    genre != 'Non-Music' & genre != 'Classical' & genre !='Stage & Screen' ,
    !str_detect(title, 'Tour|Live'),
    !str_detect(title, 'New York')
    ) 
#dat_clean <- filter(dat_clean, !str_detect(title, 'Tour|Live')) # Remove more non-studio releases
# dat_clean <- filter(dat_clean, !str_detect(title, 'New York')) # Remove other live albums
# dat_clean <- filter(dat_clean, genre != 'Non-Music') # Remove non-studio release genres
# dat_clean <- filter(dat_clean, genre != 'Stage & Screen') # Remove non-studio release genres
# dat_clean <- filter(dat_clean, genre != 'Classical') # Remove non-studio release genres
# dat_clean <- filter(dat_clean, genre != 'Children\'s') # Remove non-studio release genres
#dat_clean <- mutate(dat_clean, genre = as.factor(genre))

dat_clean %>%
  ggplot() +
  geom_bar(aes(x=year))
```

```{r}

dat_clean %>% nrow()

dat_clean %>%
  group_by(genre) %>%
  count()
```

Now we're down to 556 albums
```{r}
dat_clean %>%
  mutate(genre = genre %>% fct_infreq() %>% fct_rev()) %>%
  ggplot() +
  geom_bar(aes(x=genre)) +
  coord_flip()
```

It's not suprising that there are a lot of Rock albums as the vast majority of Discogs albums are in the Rock genre. After cleaning our initial data to be just US studio releases and the 41 rows that were missing years, we're now left with **549** albums to peform our train/test split. This is a 2.6% random sample of **20861** albums.

```{r}
dat_clean %>% nrow()
```

Let's set up our Clean Data to train our model
```{r}
# Step 1: Configure Class variable 
set.seed(123)
dat_clean <- dat_clean %>% 
  mutate(
    genre = as.factor(genre)
    ) %>% 
  select(-title, -artist) %>%
  rename(Class = genre)

dat_clean <- dat_clean %>% mutate(id = row_number()) 
```

Let's split our data into Train/Test segements.
```{r}
# Step 2: Train Split
dat_train <- dat_clean %>%
 sample_frac(0.75)

dat_test <- dat_clean %>%
 anti_join(dat_train, by = 'id') %>%
 select(-id)

dat_train <- dat_train %>%
 select(-id)
```

We'll try without preprocessing first.
```{r}
# Step 3: Pre Process - Standarize Training Data
#mod_pp <- preProcess(dat_train, method = c('scale','center'))
#dat_train_pp <- predict(mod_pp, dat_train) 
dat_train_pp <- dat_train
```

Since we have uneven distributionn on our Target....let's upsample
```{r}
# Step 4: Resample as needed
dat_train_up <- upSample(x = dat_train_pp %>% select(-Class), y = dat_train_pp$Class)

dat_train_up %>%
  group_by(Class) %>%
  count()
```

Time to train now, ask questions later!
```{r}
train_control <- trainControl(
  method = 'cv',
  number = 3
)

model_ranger = train(
  dat_train_up %>% select(-Class),
  dat_train_up$Class,
  method = "ranger",
  importance = "impurity",
  trControl = train_control,
)

model_ranger
```


```{r}

```

